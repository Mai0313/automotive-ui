import type { ChatCompletionMessageParam } from "openai/resources";

import React, { useState, useRef, useCallback, useEffect } from "react";
import {
  View,
  Text,
  StyleSheet,
  TextInput,
  TouchableOpacity,
  FlatList,
  KeyboardAvoidingView,
  Platform,
  ActivityIndicator,
} from "react-native";
import { MaterialIcons } from "@expo/vector-icons";
import { SafeAreaView } from "react-native-safe-area-context";
import {
  AudioModule,
  useAudioRecorder,
  RecordingPresets,
  AudioPlayer,
  createAudioPlayer, // Import createAudioPlayer directly
  type AudioStatus, // Import AudioStatus type
} from "expo-audio";

import { chatCompletion, transcribeAudio, textToSpeech } from "../hooks/openai"; // Added textToSpeech
import { isOpenAIConfigured } from "../utils/env";
import commonStyles from "../styles/commonStyles";
import Orb from "../components/Orb";
import { useResponsiveStyles } from "../hooks/useResponsiveStyles";

interface Message {
  id: number;
  text: string;
  isUser: boolean;
  timestamp: string;
}

const AIAssistantScreen: React.FC = () => {
  const responsiveScale = useResponsiveStyles();

  const [messages, setMessages] = useState<Message[]>([
    {
      id: 1,
      text: isOpenAIConfigured()
        ? "æ‚¨å¥½ï¼æˆ‘æ˜¯æ‚¨çš„è»Šè¼‰ AI åŠ©ç†ã€‚æˆ‘å¯ä»¥å¹«æ‚¨æ§åˆ¶è»Šè¼›åŠŸèƒ½ã€æŸ¥è©¢è³‡è¨Šæˆ–æä¾›å¹«åŠ©ã€‚è«‹å•éœ€è¦ä»€éº¼å”åŠ©ï¼Ÿ"
        : "æ‚¨å¥½ï¼æ­¡è¿ä½¿ç”¨è»Šè¼‰ AI åŠ©ç†ã€‚ç›®å‰ AI åŠŸèƒ½éœ€è¦è¨­å®š OpenAI APIï¼Œè«‹åƒè€ƒ .env.example æª”æ¡ˆé€²è¡Œé…ç½®ã€‚",
      isUser: false,
      timestamp: "10:18 AM",
    },
  ]);
  const [inputText, setInputText] = useState<string>("");
  const [isTyping, setIsTyping] = useState<boolean>(false);
  const [abortController, setAbortController] =
    useState<AbortController | null>(null);
  const [isRecording, setIsRecording] = useState<boolean>(false);
  const [isDevMode, setIsDevMode] = useState<boolean>(false); // æ–°å¢ï¼šé–‹ç™¼æ¨¡å¼
  const audioRecorder = useAudioRecorder(RecordingPresets.HIGH_QUALITY);
  const [currentSound, setCurrentSound] = useState<AudioPlayer | null>(null); // For playing TTS audio

  const flatListRef = useRef<FlatList>(null);
  const nextIdRef = useRef(2); // Start from 2 since initial message has id 1

  // Auto-scroll to bottom when messages change
  useEffect(() => {
    if (flatListRef.current && messages.length > 0) {
      setTimeout(() => {
        flatListRef.current?.scrollToEnd({ animated: true });
      }, 100);
    }
  }, [messages]);

  // Abort ongoing request if component unmounts
  useEffect(() => {
    return () => {
      if (abortController) {
        abortController.abort();
      }
      // Clean up sound object when component unmounts
      currentSound?.release();
    };
  }, [abortController, currentSound]);

  // Request microphone permissions when component mounts
  useEffect(() => {
    (async () => {
      if (Platform.OS !== "web") {
        // Only run on native platforms
        const { status } = await AudioModule.requestPermissionsAsync();

        if (status !== "granted") {
          alert("ç„¡æ³•å–å¾—éº¥å…‹é¢¨æ¬Šé™ï¼ŒéŒ„éŸ³åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨ã€‚");
        }
      }
    })();
  }, []);

  // Generate current time in HH:MM AM/PM format
  const getCurrentTime = () => {
    return new Date().toLocaleTimeString([], {
      hour: "2-digit",
      minute: "2-digit",
    });
  };

  // Function to cancel ongoing request
  const cancelRequest = useCallback(() => {
    if (abortController) {
      abortController.abort();
      setAbortController(null);
      setIsTyping(false);
    }
  }, [abortController]);

  // Function to send a message to the AI
  const sendMessage = async (textToSend?: string) => {
    const currentText = textToSend || inputText;

    if (!currentText.trim() || isTyping) return;

    // Check OpenAI configuration before proceeding
    if (!isOpenAIConfigured()) {
      console.warn(
        "ğŸš« [AIAssistant] OpenAI not configured, showing configuration message",
      );

      // Add user message
      const userMessage: Message = {
        id: nextIdRef.current++,
        text: currentText,
        isUser: true,
        timestamp: getCurrentTime(),
      };

      // Add configuration error response
      const configErrorMessage: Message = {
        id: nextIdRef.current++,
        text: "âŒ AI åŠŸèƒ½ç›®å‰ç„¡æ³•ä½¿ç”¨ï¼Œè«‹è¨­å®š OpenAI API é‡‘é‘°ã€‚è«‹åƒè€ƒå°ˆæ¡ˆçš„ .env.example æª”æ¡ˆäº†è§£å¦‚ä½•é…ç½®ç’°å¢ƒè®Šæ•¸ã€‚",
        isUser: false,
        timestamp: getCurrentTime(),
      };

      setMessages((prevMessages) => [
        ...prevMessages,
        userMessage,
        configErrorMessage,
      ]);

      if (!textToSend) {
        setInputText("");
      }

      return;
    }

    // Cancel any ongoing request
    if (abortController) {
      cancelRequest();
    }

    // Add user message
    const userMessage: Message = {
      id: nextIdRef.current++,
      text: currentText, // Use currentText instead of inputText
      isUser: true,
      timestamp: getCurrentTime(),
    };

    setMessages((prevMessages) => [...prevMessages, userMessage]);
    if (!textToSend) {
      // Clear input only if it's not from voice input
      setInputText("");
    }
    setIsTyping(true);

    // Create placeholder for AI response
    const aiPlaceholderId = nextIdRef.current++;
    const aiPlaceholder: Message = {
      id: aiPlaceholderId,
      text: "",
      isUser: false,
      timestamp: getCurrentTime(),
    };

    setMessages((prevMessages) => [...prevMessages, aiPlaceholder]);

    // Create abort controller for this request
    const controller = new AbortController();

    setAbortController(controller);

    try {
      // Prepare context with conversation history
      const conversationHistory: ChatCompletionMessageParam[] = messages
        .concat(userMessage) // Ensure userMessage is included for context
        .filter(
          (msg) =>
            msg.text.trim() !== "[æ­£åœ¨è¾¨è­˜èªéŸ³...]" &&
            msg.text.trim() !== "[èªéŸ³è¾¨è­˜å¤±æ•—]",
        ) // Filter out placeholders
        .map((msg) => ({
          role: msg.isUser ? "user" : "assistant",
          content: msg.text,
        })) as ChatCompletionMessageParam[];

      let accumulatedResponse = ""; // Accumulate full AI response for TTS

      // Start streaming response
      await chatCompletion({
        messages: conversationHistory,
        signal: controller.signal,
        onDelta: (delta: string) => {
          accumulatedResponse += delta;
          // Update AI response with streaming text
          setMessages((prevMessages) =>
            prevMessages.map((msg) =>
              msg.id === aiPlaceholderId
                ? { ...msg, text: msg.text + delta }
                : msg,
            ),
          );
        },
      });

      // After streaming is complete, play TTS if response is not empty
      if (accumulatedResponse.trim()) {
        try {
          if (currentSound) {
            console.log("[SendMessage] Stopping and releasing previous sound.");
            await currentSound.pause();
            currentSound.release();
          }
          console.log("[SendMessage] Attempting to call textToSpeech.");
          const audioUri = await textToSpeech(accumulatedResponse);

          if (audioUri) {
            const player = createAudioPlayer({ uri: audioUri });

            setCurrentSound(player);
            await player.play();
            const subscription = player.addListener(
              "playbackStatusUpdate",
              (status: AudioStatus) => {
                // Add AudioStatus type
                if (status.didJustFinish) {
                  console.log("TTS playback finished.");
                  player.release();
                  setCurrentSound(null);
                  subscription.remove(); // Clean up listener
                }
              },
            );
          } else {
            console.warn("TTS audio URI is null, cannot play.");
          }
        } catch (ttsError) {
          console.error("ğŸš« [SendMessage] TTS failed:", ttsError);
          // Don't show error to user, just log it
        }
      }
    } catch (error) {
      // Handle errors - only show error if not aborted
      if ((error as Error).name !== "AbortError") {
        setMessages((prevMessages) =>
          prevMessages.map((msg) =>
            msg.id === aiPlaceholderId
              ? {
                  ...msg,
                  text: "æŠ±æ­‰ï¼Œæˆ‘ç„¡æ³•è™•ç†æ‚¨çš„è«‹æ±‚ã€‚è«‹ç¨å¾Œå†è©¦ã€‚",
                }
              : msg,
          ),
        );
        console.error("AI Response error:", error);
      }
    } finally {
      setIsTyping(false);
      setAbortController(null);
      console.log(
        "[SendMessage] Main finally block reached. isTyping set to false.",
      );
    }
  };

  // Function to start recording audio
  const startRecording = async () => {
    console.log("[startRecording] å‡½æ•¸è¢«å‘¼å«");
    if (Platform.OS === "web") {
      // For web, check if MediaRecorder is available, which expo-audio uses under the hood.
      // Also, ensure the site is served over HTTPS for microphone access.
      if (typeof MediaRecorder === "undefined") {
        alert("ç€è¦½å™¨ä¸æ”¯æ´éŒ„éŸ³åŠŸèƒ½ã€‚");

        return;
      }
      if (
        window.location.protocol !== "https:" &&
        window.location.hostname !== "localhost" &&
        window.location.hostname !== "127.0.0.1"
      ) {
        alert("éº¥å…‹é¢¨éŒ„éŸ³éœ€è¦åœ¨ HTTPS å®‰å…¨é€£ç·šä¸‹åŸ·è¡Œã€‚");

        return;
      }
      // On web, permission is typically requested by the browser when prepareToRecordAsync or record is called.
      // So, we skip the explicit AudioModule.getPermissionsAsync/requestPermissionsAsync calls here.
    } else {
      // Native platforms still use AudioModule for permissions
      const { granted } = await AudioModule.getPermissionsAsync();

      if (!granted) {
        const { status } = await AudioModule.requestPermissionsAsync();

        if (status !== "granted") {
          alert("ç„¡æ³•å–å¾—éº¥å…‹é¢¨æ¬Šé™");

          return;
        }
      }
    }

    try {
      console.log("Starting recording with expo-audio...");
      // Ensure recorder is not already in a recording state or has a pending operation
      if (audioRecorder.isRecording || audioRecorder.uri) {
        console.log(
          "Recorder is busy or has a previous recording, resetting...",
        );
        // Attempt to stop and unload if there's a URI, or just reset state
        if (audioRecorder.isRecording) {
          await audioRecorder.stop();
        }
        // It's good practice to ensure the recorder is in a clean state before preparing a new recording.
        // The expo-audio hook should handle much of this, but explicit checks can help.
      }
      await audioRecorder.prepareToRecordAsync();
      await audioRecorder.record();
      setIsRecording(true);
      console.log("Recording started with expo-audio");
    } catch (err) {
      console.error("Failed to start recording with expo-audio", err);
      alert("éŒ„éŸ³å¤±æ•—ï¼Œè«‹æª¢æŸ¥æ¬Šé™æˆ–è£ç½®ç‹€æ…‹ã€‚");
      setIsRecording(false); // Ensure isRecording is reset on error
    }
  };

  // Function to stop recording audio and transcribe
  const stopRecordingAndTranscribe = async () => {
    console.log("[stopRecordingAndTranscribe] å‡½æ•¸è¢«å‘¼å«");
    let transcriptionPlaceholderId: number | null = null; // Declare here for wider scope

    // Check OpenAI configuration before starting transcription
    if (!isOpenAIConfigured()) {
      console.warn(
        "ğŸš« [AIAssistant] OpenAI not configured, cannot transcribe audio",
      );
      setIsRecording(false);

      // Add error message
      const errorMessage: Message = {
        id: nextIdRef.current++,
        text: "âŒ èªéŸ³è¾¨è­˜åŠŸèƒ½éœ€è¦ OpenAI API é…ç½®ã€‚",
        isUser: false,
        timestamp: getCurrentTime(),
      };

      setMessages((prevMessages) => [...prevMessages, errorMessage]);

      return;
    }

    if (!audioRecorder.isRecording) {
      if (audioRecorder.uri) {
        // If already stopped but URI exists
        console.log("Recording was already stopped. URI:", audioRecorder.uri);
      } else {
        console.log("Recorder is not active and no URI available.");
        setIsRecording(false); // Ensure UI consistency

        return;
      }
    } else {
      console.log("Stopping recording with expo-audio..");
      try {
        await audioRecorder.stop();
        console.log("Recording stopped. URI:", audioRecorder.uri);
      } catch (error) {
        console.error("Failed to stop recording:", error);
        alert("åœæ­¢éŒ„éŸ³å¤±æ•—ã€‚");
        setIsRecording(false);

        return;
      }
    }

    setIsRecording(false);
    const uri = audioRecorder.uri;

    if (uri) {
      try {
        setIsTyping(true);
        transcriptionPlaceholderId = nextIdRef.current++; // Assign here
        const transcriptionPlaceholder: Message = {
          id: transcriptionPlaceholderId,
          text: "[æ­£åœ¨è¾¨è­˜èªéŸ³...]",
          isUser: true,
          timestamp: getCurrentTime(),
        };

        setMessages((prevMessages) => [
          ...prevMessages,
          transcriptionPlaceholder,
        ]);

        const transcribedText = await transcribeAudio(uri);

        console.log("Transcribed text:", transcribedText);

        // è¾¨è­˜å®Œå¾Œï¼Œç§»é™¤ placeholderï¼Œå†å‘¼å« sendMessage
        setMessages((prevMessages) =>
          prevMessages.filter((msg) => msg.id !== transcriptionPlaceholderId),
        );

        if (transcribedText.trim()) {
          await sendMessage(transcribedText);
        } else {
          setIsTyping(false);
        }
      } catch (error) {
        console.error("ğŸš« [Transcription] Error:", error);
        setMessages((prevMessages) =>
          prevMessages.map((msg) =>
            transcriptionPlaceholderId && msg.id === transcriptionPlaceholderId // Check if ID was set
              ? {
                  ...msg,
                  text: "âŒ èªéŸ³è¾¨è­˜å¤±æ•—ï¼Œè«‹é‡è©¦æˆ–æª¢æŸ¥ OpenAI API é…ç½®ã€‚",
                }
              : msg,
          ),
        );
        setIsTyping(false);
      }
    } else {
      console.log("No recording URI found after stopping.");
      setIsTyping(false); // Ensure typing indicator is turned off
    }
  };

  const renderMessage = ({ item }: { item: Message }) => (
    <View
      style={[
        styles.messageBubble,
        item.isUser ? styles.userMessage : styles.aiMessage,
      ]}
    >
      <Text style={styles.messageText}>{item.text}</Text>
    </View>
  );

  return (
    <SafeAreaView style={commonStyles.container}>
      {/* ç›´æ¥é¡¯ç¤º Orb èªéŸ³åŠ©ç†ä»‹é¢ */}
      <View style={styles.orbContainer}>
        {Platform.OS === "web" ? (
          <View style={styles.orbWrapper}>
            <Orb
              forceHoverState={isRecording}
              hoverIntensity={0.5}
              hue={0}
              rotateOnHover={true}
            />

            {/* éŒ„éŸ³æ§åˆ¶è¦†è“‹å±¤ */}
            <TouchableOpacity
              disabled={isTyping}
              style={styles.orbOverlay}
              onPress={() => {
                console.log(
                  "[TouchableOpacity] è¢«é»æ“Šï¼ŒisRecording:",
                  isRecording,
                );
                if (isRecording) {
                  stopRecordingAndTranscribe();
                } else {
                  startRecording();
                }
              }}
            >
              {/* ç©ºçš„ TouchableOpacityï¼Œè¦†è“‹æ•´å€‹ Orb å€åŸŸä»¥æ¥æ”¶é»æ“Š */}
            </TouchableOpacity>
          </View>
        ) : (
          // åŸç”Ÿå¹³å°çš„æ›¿ä»£ UI
          <TouchableOpacity
            disabled={isTyping}
            style={[
              styles.nativeOrbButton,
              isRecording && styles.nativeOrbButtonActive,
            ]}
            onPress={isRecording ? stopRecordingAndTranscribe : startRecording}
          >
            <MaterialIcons
              color={isRecording ? "#fff" : "#3498db"}
              name={isRecording ? "stop" : "mic"}
              size={responsiveScale.largeIconSize * 2.5}
            />
            <Text
              style={[
                styles.nativeOrbButtonText,
                isRecording && styles.nativeOrbButtonTextActive,
                { fontSize: responsiveScale.mediumFontSize },
              ]}
            >
              {isRecording
                ? "åœæ­¢éŒ„éŸ³"
                : isTyping
                  ? "AI æ­£åœ¨æ€è€ƒ..."
                  : "é–‹å§‹å°è©±"}
            </Text>
          </TouchableOpacity>
        )}

        {/* ç‹€æ…‹æŒ‡ç¤ºå™¨ */}
        {(isTyping || isRecording) && (
          <View style={styles.statusIndicator}>
            <ActivityIndicator color="#3498db" size="small" />
            <Text
              style={[
                styles.statusText,
                { fontSize: responsiveScale.smallFontSize },
              ]}
            >
              {isRecording ? "æ­£åœ¨éŒ„éŸ³ä¸­..." : "AI æ­£åœ¨å›æ‡‰ä¸­..."}
            </Text>
          </View>
        )}

        {/* é–‹ç™¼æ¨¡å¼æŒ‰éˆ• - å³ä¸Šè§’ */}
        <TouchableOpacity
          style={[
            styles.devModeButton,
            { padding: responsiveScale.mediumPadding },
          ]}
          onPress={() => setIsDevMode(true)}
        >
          <MaterialIcons
            color="#ffffff"
            name="keyboard"
            size={responsiveScale.mediumIconSize}
          />
          <Text
            style={[
              styles.devModeButtonText,
              { fontSize: responsiveScale.smallFontSize },
            ]}
          >
            é–‹ç™¼æ¨¡å¼
          </Text>
        </TouchableOpacity>
      </View>

      {/* Dev Mode å½ˆå‡ºè¦–çª— */}
      {isDevMode && (
        <View style={styles.devModeOverlay}>
          <View style={styles.devModeContainer}>
            <View style={styles.devModeHeader}>
              <Text
                style={[
                  styles.devModeTitle,
                  { fontSize: responsiveScale.largeFontSize },
                ]}
              >
                é–‹ç™¼æ¨¡å¼ - æ–‡å­—è¼¸å…¥èˆ‡èŠå¤©æ­·å²
              </Text>
              <TouchableOpacity
                style={[
                  styles.devModeCloseButton,
                  { padding: responsiveScale.smallPadding },
                ]}
                onPress={() => setIsDevMode(false)}
              >
                <MaterialIcons
                  color="#fff"
                  name="close"
                  size={responsiveScale.mediumIconSize}
                />
              </TouchableOpacity>
            </View>

            {/* èŠå¤©æ­·å²å€åŸŸ */}
            <View style={styles.chatContainer}>
              <FlatList
                ref={flatListRef}
                contentContainerStyle={styles.messagesList}
                data={messages}
                keyExtractor={(item) => item.id.toString()}
                renderItem={renderMessage}
                showsVerticalScrollIndicator={false}
              />
            </View>

            <KeyboardAvoidingView
              behavior={Platform.OS === "ios" ? "padding" : "height"}
              style={styles.devInputContainer}
            >
              <View style={styles.inputRow}>
                <TextInput
                  multiline
                  editable={!isTyping && !isRecording}
                  placeholder="è«‹è¼¸å…¥è¨Šæ¯..."
                  placeholderTextColor="#777"
                  style={styles.input}
                  value={inputText}
                  onChangeText={setInputText}
                  onSubmitEditing={() => sendMessage()}
                />
                {isTyping && !isRecording ? (
                  <TouchableOpacity
                    style={[
                      styles.iconButton,
                      {
                        width: responsiveScale.buttonSize,
                        height: responsiveScale.buttonSize,
                        borderRadius: responsiveScale.buttonSize / 2,
                      },
                    ]}
                    onPress={cancelRequest}
                  >
                    <MaterialIcons
                      color="#e74c3c"
                      name="close"
                      size={responsiveScale.mediumIconSize}
                    />
                  </TouchableOpacity>
                ) : !isRecording ? (
                  <TouchableOpacity
                    disabled={!inputText.trim() || isTyping}
                    style={[
                      styles.iconButton,
                      {
                        width: responsiveScale.buttonSize,
                        height: responsiveScale.buttonSize,
                        borderRadius: responsiveScale.buttonSize / 2,
                      },
                      (!inputText.trim() || isTyping) &&
                        styles.iconButtonDisabled,
                    ]}
                    onPress={() => sendMessage()}
                  >
                    <MaterialIcons
                      color="#3498db"
                      name="send"
                      size={responsiveScale.mediumIconSize}
                    />
                  </TouchableOpacity>
                ) : null}
              </View>
              {(isTyping || isRecording) && (
                <View style={styles.typingIndicator}>
                  <ActivityIndicator color="#3498db" size="small" />
                  <Text
                    style={[
                      styles.typingText,
                      { fontSize: responsiveScale.smallFontSize },
                    ]}
                  >
                    {isRecording
                      ? "æ­£åœ¨éŒ„éŸ³ä¸­..."
                      : isTyping
                        ? "AI æ­£åœ¨å›æ‡‰ä¸­..."
                        : ""}
                  </Text>
                </View>
              )}
            </KeyboardAvoidingView>
          </View>
        </View>
      )}
    </SafeAreaView>
  );
};

const styles = StyleSheet.create({
  // Message styles (still needed for dev mode)
  messageBubble: {
    marginBottom: 15,
    maxWidth: "80%",
    borderRadius: 20,
    padding: 12,
    paddingVertical: 10,
    flexDirection: "row",
  },
  userMessage: {
    backgroundColor: "#333",
    alignSelf: "flex-end",
    borderBottomRightRadius: 5,
  },
  aiMessage: {
    backgroundColor: "#222",
    alignSelf: "flex-start",
    borderBottomLeftRadius: 5,
  },
  messageText: {
    color: "#fff",
    fontSize: 16,
  },
  chatContainer: {
    flex: 1,
    paddingHorizontal: 10,
    paddingBottom: 10,
  },
  messagesList: {
    paddingVertical: 10,
  },
  devModeButton: {
    position: "absolute",
    top: 50,
    right: 20,
    flexDirection: "row",
    alignItems: "center",
    justifyContent: "center",
    backgroundColor: "rgba(34, 34, 34, 0.8)",
    borderRadius: 15,
    padding: 10,
    zIndex: 10,
  },
  devModeButtonText: {
    color: "#ffffff",
    fontSize: 14,
    marginLeft: 8,
  },
  orbContainer: {
    flex: 1,
    justifyContent: "center",
    alignItems: "center",
    padding: 20,
  },
  orbWrapper: {
    width: "100%",
    height: 400,
    position: "relative",
    justifyContent: "center",
    alignItems: "center",
  },
  orbOverlay: {
    position: "absolute",
    top: 0,
    left: 0,
    right: 0,
    bottom: 0,
    justifyContent: "center",
    alignItems: "center",
    backgroundColor: "transparent",
    zIndex: 10, // ç¢ºä¿åœ¨ Orb ä¸Šæ–¹
  },
  orbStatus: {
    position: "absolute",
    bottom: -80, // ç§»åˆ° Orb å®¹å™¨ä¸‹æ–¹
    left: 0,
    right: 0,
    alignItems: "center",
    zIndex: 5, // ä½æ–¼è¦†è“‹å±¤
  },
  orbStatusText: {
    color: "#fff",
    fontSize: 16,
    textAlign: "center",
    backgroundColor: "rgba(0,0,0,0.7)",
    padding: 10,
    borderRadius: 15,
  },
  nativeOrbButton: {
    backgroundColor: "#222",
    borderRadius: 100,
    width: 200,
    height: 200,
    justifyContent: "center",
    alignItems: "center",
    elevation: 4,
    boxShadow: "0px 2px 4px rgba(0,0,0,0.2)",
  },
  nativeOrbButtonActive: {
    backgroundColor: "#e74c3c",
  },
  nativeOrbButtonText: {
    color: "#3498db",
    fontSize: 16,
    fontWeight: "bold",
    marginTop: 12,
    textAlign: "center",
  },
  nativeOrbButtonTextActive: {
    color: "#fff",
  },
  statusIndicator: {
    flexDirection: "row",
    alignItems: "center",
    marginTop: 20,
    justifyContent: "center",
  },
  statusText: {
    color: "#3498db",
    marginLeft: 8,
    fontSize: 14,
  },
  orbControls: {
    flexDirection: "row",
    justifyContent: "space-around",
    width: "100%",
    marginTop: 40,
  },
  controlButton: {
    flexDirection: "row",
    alignItems: "center",
    backgroundColor: "#222",
    borderRadius: 15,
    padding: 12,
    paddingHorizontal: 16,
  },
  controlButtonText: {
    color: "#ffffff",
    fontSize: 14,
    marginLeft: 8,
  },
  devModeOverlay: {
    position: "absolute",
    top: 0,
    left: 0,
    right: 0,
    bottom: 0,
    backgroundColor: "rgba(0,0,0,0.9)",
    justifyContent: "center",
    alignItems: "center",
    zIndex: 1000,
  },
  devModeContainer: {
    backgroundColor: "#111",
    borderRadius: 20,
    padding: 20,
    width: "90%",
    maxHeight: "80%",
  },
  devModeHeader: {
    flexDirection: "row",
    justifyContent: "space-between",
    alignItems: "center",
    marginBottom: 20,
  },
  devModeTitle: {
    color: "#fff",
    fontSize: 18,
    fontWeight: "bold",
  },
  devModeCloseButton: {
    padding: 5,
  },
  devInputContainer: {
    flex: 1,
  },
  inputContainer: {
    paddingHorizontal: 10,
    paddingVertical: 10,
    borderTopWidth: 1,
    borderTopColor: "#222",
  },
  inputRow: {
    flexDirection: "row",
    alignItems: "flex-end",
  },
  input: {
    flex: 1,
    backgroundColor: "#222",
    color: "#fff",
    padding: 12,
    borderRadius: 15,
    marginRight: 10,
    fontSize: 16,
    maxHeight: 100,
  },
  iconButton: {
    width: 45,
    height: 45,
    borderRadius: 25,
    backgroundColor: "#222",
    justifyContent: "center",
    alignItems: "center",
    marginLeft: 5,
  },
  iconButtonDisabled: {
    opacity: 0.5,
  },
  typingIndicator: {
    flexDirection: "row",
    alignItems: "center",
    marginTop: 8,
    marginLeft: 15,
  },
  typingText: {
    color: "#3498db",
    marginLeft: 8,
    fontSize: 14,
  },
});

export default AIAssistantScreen;
