import type { ChatCompletionMessageParam } from "openai/resources";

import { useState, useCallback, useRef, useEffect } from "react";
import { Platform } from "react-native";
import { Audio } from "expo-av";

import { getStreamingTTSUrl } from "../utils/env";

import { chatCompletion } from "./openai";

export interface RealtimeTTSState {
  isProcessing: boolean;
  isConnected: boolean;
  isSpeaking: boolean;
  error: string | null;
  currentText: string;
  processedText: string;
}

export interface RealtimeTTSConfig {
  onStatusChange?: (status: RealtimeTTSState) => void;
  onError?: (error: string) => void;
  onSpeakingComplete?: () => void;
}

export const useRealtimeTTS = (config?: RealtimeTTSConfig) => {
  const [state, setState] = useState<RealtimeTTSState>({
    isProcessing: false,
    isConnected: false,
    isSpeaking: false,
    error: null,
    currentText: "",
    processedText: "",
  });

  const wsRef = useRef<WebSocket | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const currentPlaybackRef = useRef<Audio.Sound | null>(null);
  const pendingAudioChunks = useRef<ArrayBuffer[]>([]);
  const isFlushingRef = useRef(false);
  const currentAudioSourceRef = useRef<AudioBufferSourceNode | null>(null);
  const currentHtmlAudioRef = useRef<HTMLAudioElement | null>(null);
  const audioQueueRef = useRef<ArrayBuffer[]>([]);
  const isPlayingQueueRef = useRef(false);

  // åˆå§‹åŒ– AudioContext (Web only)
  const initializeAudioContext = useCallback(() => {
    if (Platform.OS === "web" && !audioContextRef.current) {
      try {
        audioContextRef.current = new (window.AudioContext ||
          (window as any).webkitAudioContext)();
        console.log("ğŸ”Š [RealtimeTTS] AudioContext initialized");
      } catch (error) {
        console.error(
          "ğŸš« [RealtimeTTS] Failed to initialize AudioContext:",
          error,
        );
      }
    }
  }, []);

  // æª¢æŸ¥éŸ³è¨Šè³‡æ–™æ ¼å¼
  const detectAudioFormat = useCallback((data: ArrayBuffer): string => {
    const bytes = new Uint8Array(data);

    // æª¢æŸ¥ WAV æ ¼å¼ (RIFF header)
    if (
      bytes.length >= 4 &&
      bytes[0] === 0x52 &&
      bytes[1] === 0x49 &&
      bytes[2] === 0x46 &&
      bytes[3] === 0x46
    ) {
      return "wav";
    }

    // æª¢æŸ¥ MP3 æ ¼å¼ (ID3 tag æˆ– frame sync)
    if (
      bytes.length >= 3 &&
      ((bytes[0] === 0x49 && bytes[1] === 0x44 && bytes[2] === 0x33) || // ID3
        (bytes[0] === 0xff && (bytes[1] & 0xe0) === 0xe0))
    ) {
      // Frame sync
      return "mp3";
    }

    // æª¢æŸ¥ OGG æ ¼å¼
    if (
      bytes.length >= 4 &&
      bytes[0] === 0x4f &&
      bytes[1] === 0x67 &&
      bytes[2] === 0x67 &&
      bytes[3] === 0x53
    ) {
      return "ogg";
    }

    // å…¶ä»–æƒ…æ³å‡è¨­ç‚ºåŸå§‹ PCM
    return "pcm";
  }, []);

  // å°‡åŸå§‹ PCM è³‡æ–™è½‰æ›ç‚º WAV æ ¼å¼
  const createWavFromPCM = useCallback(
    (
      pcmData: ArrayBuffer,
      sampleRate: number = 22050,
      channels: number = 1,
    ): ArrayBuffer => {
      const pcmLength = pcmData.byteLength;
      const wavLength = 44 + pcmLength; // WAV header (44 bytes) + PCM data
      const buffer = new ArrayBuffer(wavLength);
      const view = new DataView(buffer);
      const pcmView = new Int16Array(pcmData);

      // WAV header
      const writeString = (offset: number, string: string) => {
        for (let i = 0; i < string.length; i++) {
          view.setUint8(offset + i, string.charCodeAt(i));
        }
      };

      writeString(0, "RIFF");
      view.setUint32(4, wavLength - 8, true); // Chunk size
      writeString(8, "WAVE");
      writeString(12, "fmt ");
      view.setUint32(16, 16, true); // Subchunk1 size
      view.setUint16(20, 1, true); // Audio format (PCM)
      view.setUint16(22, channels, true); // Number of channels
      view.setUint32(24, sampleRate, true); // Sample rate
      view.setUint32(28, sampleRate * channels * 2, true); // Byte rate
      view.setUint16(32, channels * 2, true); // Block align
      view.setUint16(34, 16, true); // Bits per sample
      writeString(36, "data");
      view.setUint32(40, pcmLength, true); // Subchunk2 size

      // Copy PCM data
      const wavPcm = new Int16Array(buffer, 44);

      wavPcm.set(pcmView);

      return buffer;
    },
    [],
  );

  // åœæ­¢ç•¶å‰æ’­æ”¾çš„éŸ³è¨Š
  const stopCurrentAudio = useCallback(() => {
    // åœæ­¢ AudioContext æ’­æ”¾
    if (currentAudioSourceRef.current) {
      try {
        currentAudioSourceRef.current.stop();
        currentAudioSourceRef.current.disconnect();
      } catch (error) {
        console.error(
          `ğŸš« [RealtimeTTS] Error stopping AudioContext source: ${error}`,
        );
      }
      currentAudioSourceRef.current = null;
    }

    // åœæ­¢ HTML Audio æ’­æ”¾
    if (currentHtmlAudioRef.current) {
      try {
        currentHtmlAudioRef.current.pause();
        currentHtmlAudioRef.current.currentTime = 0;
      } catch (error) {
        console.error(`ğŸš« [RealtimeTTS] Error stopping HTML Audio:`, error);
      }
      currentHtmlAudioRef.current = null;
    }
  }, []);

  // è™•ç†éŸ³è¨Šä½‡åˆ—
  const processAudioQueue = useCallback(async () => {
    if (isPlayingQueueRef.current || audioQueueRef.current.length === 0) {
      return;
    }

    isPlayingQueueRef.current = true;

    while (audioQueueRef.current.length > 0) {
      const audioData = audioQueueRef.current.shift();

      if (audioData) {
        await playAudioChunkInternal(audioData);
        // å°å»¶é²é¿å…éŸ³è¨Šé‡ç–Š
        await new Promise((resolve) => setTimeout(resolve, 50));
      }
    }

    isPlayingQueueRef.current = false;
  }, []);

  // å…§éƒ¨éŸ³è¨Šæ’­æ”¾å‡½æ•¸ï¼ˆä¸é€éä½‡åˆ—ï¼‰
  const playAudioChunkInternal = useCallback(
    async (audioData: ArrayBuffer) => {
      try {
        if (Platform.OS === "web" && audioContextRef.current) {
          // é¦–å…ˆæª¢æŸ¥éŸ³è¨Šè³‡æ–™æ˜¯å¦æœ‰æ•ˆ
          if (audioData.byteLength === 0) {
            console.warn("ğŸš« [RealtimeTTS] Empty audio data received");

            return;
          }

          // åœæ­¢ä¹‹å‰çš„æ’­æ”¾
          stopCurrentAudio();

          const audioFormat = detectAudioFormat(audioData);

          console.log(
            `ğŸ”Š [RealtimeTTS] Detected audio format: ${audioFormat}, size: ${audioData.byteLength} bytes`,
          );

          let processedAudioData = audioData;

          // å¦‚æœæ˜¯åŸå§‹ PCMï¼Œå…ˆè½‰æ›ç‚º WAV
          if (audioFormat === "pcm") {
            console.log("ğŸ”§ [RealtimeTTS] Converting PCM to WAV format");
            processedAudioData = createWavFromPCM(audioData);
          }

          // å˜—è©¦ä½¿ç”¨ AudioContext è§£ç¢¼
          try {
            const audioBuffer = await audioContextRef.current.decodeAudioData(
              processedAudioData.slice(0), // å‰µå»º copy é¿å… detached buffer
            );

            const source = audioContextRef.current.createBufferSource();

            source.buffer = audioBuffer;
            source.connect(audioContextRef.current.destination);

            // ä¿å­˜ç•¶å‰æ’­æ”¾æºçš„å¼•ç”¨
            currentAudioSourceRef.current = source;

            // æ›´æ–°æ’­æ”¾ç‹€æ…‹
            setState((prev) => ({ ...prev, isSpeaking: true }));

            source.start();
            console.log(
              "âœ… [RealtimeTTS] Audio chunk started playing via AudioContext",
            );

            // ç­‰å¾…æ’­æ”¾å®Œæˆ
            await new Promise<void>((resolve) => {
              source.onended = () => {
                if (currentAudioSourceRef.current === source) {
                  setState((prev) => ({ ...prev, isSpeaking: false }));
                  currentAudioSourceRef.current = null;
                  config?.onSpeakingComplete?.();
                }
                resolve();
              };
            });

            return; // æˆåŠŸæ’­æ”¾ï¼Œç›´æ¥è¿”å›
          } catch (decodeError) {
            console.error(
              "ğŸš« [RealtimeTTS] AudioContext decode error:",
              decodeError,
            );
            console.log("ğŸ”§ [RealtimeTTS] Audio data details:", {
              originalSize: audioData.byteLength,
              processedSize: processedAudioData.byteLength,
              format: audioFormat,
              firstBytes:
                audioData.byteLength > 0
                  ? Array.from(new Uint8Array(audioData.slice(0, 16)))
                  : "empty",
            });
          }

          // AudioContext å¤±æ•—ï¼Œå˜—è©¦ HTML Audio å…ƒç´  fallback
          try {
            console.log("ğŸ”§ [RealtimeTTS] Trying HTML Audio fallback");

            // å˜—è©¦å¤šç¨® MIME types
            const mimeTypes =
              audioFormat === "pcm"
                ? ["audio/wav", "audio/wave"]
                : audioFormat === "mp3"
                  ? ["audio/mpeg", "audio/mp3"]
                  : audioFormat === "ogg"
                    ? ["audio/ogg"]
                    : ["audio/wav", "audio/mpeg", "audio/ogg"];

            for (const mimeType of mimeTypes) {
              try {
                const blob = new Blob([processedAudioData], { type: mimeType });
                const audioUrl = URL.createObjectURL(blob);
                const htmlAudio = new (window as any).Audio(audioUrl);

                // ä¿å­˜ç•¶å‰ HTML Audio å¼•ç”¨
                currentHtmlAudioRef.current = htmlAudio;

                await new Promise<void>((resolve, reject) => {
                  htmlAudio.onloadeddata = () => {
                    setState((prev) => ({ ...prev, isSpeaking: true }));
                  };

                  htmlAudio.onended = () => {
                    if (currentHtmlAudioRef.current === htmlAudio) {
                      setState((prev) => ({ ...prev, isSpeaking: false }));
                      currentHtmlAudioRef.current = null;
                      config?.onSpeakingComplete?.();
                    }
                    URL.revokeObjectURL(audioUrl);
                    resolve();
                  };

                  htmlAudio.onerror = (audioError: Event) => {
                    console.error(
                      `ğŸš« [RealtimeTTS] HTML Audio error with ${mimeType}:`,
                      audioError,
                    );
                    URL.revokeObjectURL(audioUrl);
                    if (currentHtmlAudioRef.current === htmlAudio) {
                      currentHtmlAudioRef.current = null;
                    }
                    reject(audioError);
                  };

                  htmlAudio
                    .play()
                    .then(() => {
                      console.log(
                        `âœ… [RealtimeTTS] HTML Audio playback successful with ${mimeType}`,
                      );
                    })
                    .catch(reject);
                });

                return; // æˆåŠŸæ’­æ”¾ï¼Œé€€å‡º
              } catch (mimeError) {
                console.warn(
                  `ğŸš« [RealtimeTTS] Failed with MIME type ${mimeType}:`,
                  mimeError,
                );
                continue;
              }
            }

            throw new Error("All HTML Audio fallback attempts failed");
          } catch (fallbackError) {
            console.error(
              "ğŸš« [RealtimeTTS] All audio playback methods failed:",
              fallbackError,
            );

            // æœ€å¾Œå˜—è©¦ï¼šå„²å­˜éŸ³è¨Šè³‡æ–™ä¾›èª¿è©¦
            if (audioData.byteLength > 0) {
              const blob = new Blob([processedAudioData], {
                type: "application/octet-stream",
              });
              const url = URL.createObjectURL(blob);

              console.log(
                "ğŸ”§ [RealtimeTTS] Debug: Audio data URL for manual inspection:",
                url,
              );

              // 5 ç§’å¾Œæ¸…é™¤ URL
              setTimeout(() => URL.revokeObjectURL(url), 5000);
            }
          }
        } else {
          // åŸç”Ÿå¹³å°ä½¿ç”¨ Expo Audio
          console.warn(
            "ğŸ”Š [RealtimeTTS] Native platform audio playback not fully implemented",
          );
        }
      } catch (error) {
        console.error("ğŸš« [RealtimeTTS] Audio playback error:", error);
      }
    },
    [config, detectAudioFormat, createWavFromPCM, stopCurrentAudio],
  );

  // å…¬é–‹çš„éŸ³è¨Šæ’­æ”¾å‡½æ•¸ï¼ˆé€éä½‡åˆ—ï¼‰
  const playAudioChunk = useCallback(
    async (audioData: ArrayBuffer) => {
      // å°‡éŸ³è¨ŠåŠ å…¥ä½‡åˆ—
      audioQueueRef.current.push(audioData);

      // é–‹å§‹è™•ç†ä½‡åˆ—
      await processAudioQueue();
    },
    [processAudioQueue],
  );

  // é€£æ¥åˆ° WebSocket TTS æœå‹™
  const connect = useCallback(async () => {
    if (wsRef.current?.readyState === WebSocket.OPEN) {
      return; // å·²ç¶“é€£æ¥
    }

    try {
      initializeAudioContext();

      const wsUrl = getStreamingTTSUrl();

      console.log("ğŸ”Œ [RealtimeTTS] Connecting to:", wsUrl);

      wsRef.current = new WebSocket(wsUrl);

      wsRef.current.onopen = () => {
        console.log("âœ… [RealtimeTTS] WebSocket connected");
        setState((prev) => ({
          ...prev,
          isConnected: true,
          error: null,
        }));
        config?.onStatusChange?.(state);
      };

      wsRef.current.onmessage = async (event) => {
        if (event.data instanceof Blob) {
          // æ¥æ”¶åˆ°éŸ³é »æ•¸æ“š
          const arrayBuffer = await event.data.arrayBuffer();

          // å°‡éŸ³è¨Šå¡Šæ·»åŠ åˆ°éšŠåˆ—
          audioQueueRef.current.push(arrayBuffer);
          pendingAudioChunks.current.push(arrayBuffer);

          // å¦‚æœç•¶å‰æ²’æœ‰æ’­æ”¾ï¼Œé–‹å§‹è™•ç†éšŠåˆ—
          if (!isPlayingQueueRef.current) {
            processAudioQueue();
          }
        } else {
          // æ¥æ”¶åˆ°æ–‡å­—æ¶ˆæ¯ (JSON)
          try {
            const message = JSON.parse(event.data);

            console.log("ğŸ“¨ [RealtimeTTS] Received message:", message);

            switch (message.type) {
              case "chunk_complete":
                setState((prev) => ({
                  ...prev,
                  processedText: prev.processedText + message.text,
                }));
                break;
              case "flush_complete":
                setState((prev) => ({
                  ...prev,
                  processedText: prev.currentText,
                  isProcessing: false,
                }));
                isFlushingRef.current = false;
                break;
              case "reset_complete":
                setState((prev) => ({
                  ...prev,
                  currentText: "",
                  processedText: "",
                  isProcessing: false,
                }));
                break;
              case "error":
                console.error("ğŸš« [RealtimeTTS] Server error:", message.error);
                setState((prev) => ({
                  ...prev,
                  error: message.error,
                  isProcessing: false,
                }));
                config?.onError?.(message.error);
                break;
            }
          } catch (e) {
            console.error("ğŸš« [RealtimeTTS] Failed to parse message:", e);
          }
        }
      };

      wsRef.current.onclose = () => {
        console.log("ğŸ”Œ [RealtimeTTS] WebSocket disconnected");
        setState((prev) => ({
          ...prev,
          isConnected: false,
        }));
      };

      wsRef.current.onerror = (error) => {
        console.error("ğŸš« [RealtimeTTS] WebSocket error:", error);
        setState((prev) => ({
          ...prev,
          isConnected: false,
          error: "WebSocket connection failed",
        }));
        config?.onError?.("WebSocket connection failed");
      };
    } catch (error) {
      console.error("ğŸš« [RealtimeTTS] Connection error:", error);
      setState((prev) => ({
        ...prev,
        error: error instanceof Error ? error.message : "Connection failed",
      }));
      config?.onError?.(
        error instanceof Error ? error.message : "Connection failed",
      );
    }
  }, [initializeAudioContext, playAudioChunk, config, state]);

  // ç™¼é€æ–‡å­—åˆ° TTS æœå‹™
  const sendText = useCallback((text: string) => {
    if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {
      console.warn("ğŸš« [RealtimeTTS] WebSocket not connected");

      return;
    }

    try {
      // ç™¼é€ JSON æ¶ˆæ¯
      const message = {
        type: "text",
        text: text,
      };

      wsRef.current.send(JSON.stringify(message));
      console.log("ğŸ“¤ [RealtimeTTS] Sent text:", text);

      setState((prev) => ({
        ...prev,
        currentText: prev.currentText + text,
      }));
    } catch (error) {
      console.error("ğŸš« [RealtimeTTS] Failed to send text:", error);
    }
  }, []);

  // å¼·åˆ¶è™•ç†å‰©é¤˜æ–‡å­—
  const flush = useCallback(() => {
    if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {
      return;
    }

    try {
      isFlushingRef.current = true;
      wsRef.current.send(JSON.stringify({ type: "flush" }));
      console.log("ğŸ”„ [RealtimeTTS] Flushing remaining text");
    } catch (error) {
      console.error("ğŸš« [RealtimeTTS] Failed to flush:", error);
      isFlushingRef.current = false;
    }
  }, []);

  // é‡ç½® TTS ç‹€æ…‹
  const reset = useCallback(() => {
    if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {
      return;
    }

    try {
      wsRef.current.send(JSON.stringify({ type: "reset" }));
      pendingAudioChunks.current = [];
      console.log("ğŸ”„ [RealtimeTTS] Reset TTS state");
    } catch (error) {
      console.error("ğŸš« [RealtimeTTS] Failed to reset:", error);
    }
  }, []);

  // åŸ·è¡Œå®Œæ•´çš„ Chat + TTS æµç¨‹ (éä¸²æµæ¨¡å¼)
  const processConversation = useCallback(
    async (
      messages: ChatCompletionMessageParam[],
      options?: {
        onDelta?: (delta: string) => void;
        signal?: AbortSignal;
      },
    ) => {
      if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {
        await connect();
        // ç­‰å¾…é€£æ¥å»ºç«‹
        await new Promise((resolve) => setTimeout(resolve, 1000));
      }

      setState((prev) => ({
        ...prev,
        isProcessing: true,
        currentText: "",
        processedText: "",
        error: null,
      }));

      try {
        // é‡ç½® TTS ç‹€æ…‹
        reset();

        console.log(
          "ğŸ”„ [RealtimeTTS] Starting chat completion (non-streaming)...",
        );

        // ä½¿ç”¨éä¸²æµæ¨¡å¼ç”Ÿæˆå®Œæ•´å›æ‡‰
        const completeResponse = await chatCompletion({
          messages,
          stream: false,
          signal: options?.signal,
        });

        // æª¢æŸ¥å›æ‡‰æ˜¯å¦æœ‰æ•ˆ
        if (!completeResponse || typeof completeResponse !== "string") {
          throw new Error("Failed to generate response");
        }

        console.log(
          "âœ… [RealtimeTTS] Chat completion finished, response length:",
          completeResponse.length,
        );

        // æ›´æ–°ç•¶å‰æ–‡å­—ç‹€æ…‹
        setState((prev) => ({
          ...prev,
          currentText: completeResponse,
        }));

        // å¦‚æœæœ‰ onDelta å›èª¿ï¼Œä¸€æ¬¡æ€§å‚³éå®Œæ•´å›æ‡‰
        if (options?.onDelta) {
          options.onDelta(completeResponse);
        }

        // å°‡å®Œæ•´å¥å­ç™¼é€åˆ° TTS WebSocket
        console.log("ğŸ”„ [RealtimeTTS] Sending complete response to TTS...");
        sendText(completeResponse);

        // å¼·åˆ¶è™•ç†æ–‡å­—
        flush();
      } catch (error) {
        console.error("ğŸš« [RealtimeTTS] Conversation processing error:", error);
        setState((prev) => ({
          ...prev,
          isProcessing: false,
          error: error instanceof Error ? error.message : "Processing failed",
        }));
        config?.onError?.(
          error instanceof Error ? error.message : "Processing failed",
        );
      }
    },
    [connect, reset, sendText, flush, config],
  );

  // æ–·é–‹é€£æ¥
  const disconnect = useCallback(() => {
    if (wsRef.current) {
      wsRef.current.close();
      wsRef.current = null;
    }

    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }

    if (currentPlaybackRef.current) {
      currentPlaybackRef.current.unloadAsync();
      currentPlaybackRef.current = null;
    }

    setState((prev) => ({
      ...prev,
      isConnected: false,
      isProcessing: false,
      isSpeaking: false,
    }));
  }, []);

  // æ¸…ç†å‡½æ•¸
  useEffect(() => {
    return () => {
      disconnect();
    };
  }, [disconnect]);

  return {
    state,
    connect,
    disconnect,
    sendText,
    flush,
    reset,
    processConversation,
    isConnected: state.isConnected,
    isProcessing: state.isProcessing,
    isSpeaking: state.isSpeaking,
    error: state.error,
  };
};
